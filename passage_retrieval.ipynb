{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# PolEval - Passage Retrieval",
   "id": "9a7d74a819ad0fcd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Imports and consts",
   "id": "976f3920290a0790"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import spacy\n",
    "# Resets variables. Execute if needed\n",
    "%reset"
   ],
   "id": "64f985435704c2af",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5083bc1b",
   "metadata": {},
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from haystack_integrations.document_stores.elasticsearch import ElasticsearchDocumentStore\n",
    "from haystack_integrations.components.retrievers.elasticsearch import ElasticsearchBM25Retriever\n",
    "from haystack import Document\n",
    "from haystack import Pipeline\n",
    "from huggingface_hub import hf_hub_download\n",
    "from datasets import load_dataset\n",
    "from haystack.document_stores.types.policy import DuplicatePolicy"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "01891994",
   "metadata": {},
   "source": "dataset_path = 'datasets/'",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Download datasets",
   "id": "1cc4e651b2327e78"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "passages_source = {\n",
    "    'allegro': hf_hub_download(repo_id=\"piotr-rybak/poleval2022-passage-retrieval-dataset\", subfolder=\"allegro-faq\", filename=\"passages.jl\", repo_type=\"dataset\"),\n",
    "    'legal': hf_hub_download(repo_id=\"piotr-rybak/poleval2022-passage-retrieval-dataset\", subfolder=\"legal-questions\", filename=\"passages.jl\", repo_type=\"dataset\"),\n",
    "    'wiki': hf_hub_download(repo_id=\"piotr-rybak/poleval2022-passage-retrieval-dataset\", subfolder=\"wiki-trivia\", filename=\"passages.jl\", repo_type=\"dataset\"),\n",
    "}\n",
    "\n",
    "questions_source = {\n",
    "    'allegro':  hf_hub_download(repo_id=\"piotr-rybak/poleval2022-passage-retrieval-dataset\", subfolder=\"allegro-faq\", filename=\"questions-test.jl\", repo_type=\"dataset\"),\n",
    "    'wiki': hf_hub_download(repo_id=\"piotr-rybak/poleval2022-passage-retrieval-dataset\", subfolder=\"wiki-trivia\", filename=\"questions-test.jl\", repo_type=\"dataset\"),\n",
    "    'legal': hf_hub_download(repo_id=\"piotr-rybak/poleval2022-passage-retrieval-dataset\", subfolder=\"legal-questions\", filename=\"questions-test.jl\", repo_type=\"dataset\"),\n",
    "    'dev-0': dataset_path + 'dev-0/in.tsv',\n",
    "    'test-A': dataset_path + 'test-A/in.tsv',\n",
    "    'test-B': dataset_path + 'test-B/in.tsv',\n",
    "}\n",
    "\n",
    "training_set = load_dataset(\"piotr-rybak/poleval2022-passage-retrieval-dataset\", split=\"train\")\n",
    "test_set = load_dataset(\"piotr-rybak/poleval2022-passage-retrieval-dataset\", split=\"test\")\n",
    "wiki_questions_train = hf_hub_download(repo_id=\"piotr-rybak/poleval2022-passage-retrieval-dataset\", subfolder=\"wiki-trivia\", filename=\"questions-train.jl\", repo_type=\"dataset\")"
   ],
   "id": "1ea8d67008a70c1a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d704ae67",
   "metadata": {},
   "source": [
    "## Launch ElasticSearch in Docker and create stores\n",
    "\n",
    "In terminal when in root directory run:\n",
    "- docker-compose up"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Count documents in store to verify\n",
    "def count_documents(stores):\n",
    "    for key in stores:\n",
    "        print(stores[key].count_documents())"
   ],
   "id": "a9bcae1c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Declare stores\n",
    "import requests\n",
    "import json\n",
    "\n",
    "document_stores = {\n",
    "    'allegro': ElasticsearchDocumentStore(hosts='http://localhost:9200', index='allegro'),\n",
    "    'wiki': ElasticsearchDocumentStore(hosts='http://localhost:9200', index='wiki'),\n",
    "    'legal': ElasticsearchDocumentStore(hosts='http://localhost:9200', index='legal'),\n",
    "}\n",
    "\n",
    "def change_document_store_window_size(key, size):\n",
    "    url = \"http://localhost:9200/\" + key + \"/_settings\"\n",
    "    \n",
    "    data = {\n",
    "      \"index\": {\n",
    "        \"max_result_window\" : size\n",
    "      }\n",
    "    }\n",
    "    \n",
    "    headers = {\n",
    "        \"content-type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    response = requests.put(url, headers=headers, data=json.dumps(data))\n",
    "    \n",
    "    print(response.text)\n",
    "\n",
    "change_document_store_window_size('wiki', 300000)\n",
    "change_document_store_window_size('legal', 300000)"
   ],
   "id": "432a17cdddb1f241",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Function which deletes all documents from the document stores if no argument is passed. Deletes 10000 docs at a time. If store is passed, deletes 10k documents from particular store.\n",
    "\"\"\"\n",
    "def delete_documents(store=None):\n",
    "    if store is not None:\n",
    "        store.delete_documents(document_ids=list(map(lambda document: document.id, store.filter_documents(filters={}))))\n",
    "    else:\n",
    "        count_documents(document_stores)\n",
    "        \n",
    "        for key in document_stores:\n",
    "            documents_to_delete = list(map(lambda document: document.id, document_stores[key].filter_documents(filters={})))\n",
    "            while len(documents_to_delete) > 0:\n",
    "                print(\"Deleting...\")\n",
    "                document_stores[key].delete_documents(document_ids=documents_to_delete)\n",
    "                documents_to_delete = list(map(lambda document: document.id, document_stores[key].filter_documents(filters={})))\n",
    "                \n",
    "        count_documents(document_stores)"
   ],
   "id": "d8016ba6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "count_documents(document_stores)",
   "id": "391eb01701d82549",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Index Passages",
   "id": "093bb8c2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "passages = {\n",
    "    'allegro': pd.read_json(passages_source['allegro'], lines=True, chunksize=int(1e5)),\n",
    "    'wiki': pd.read_json(passages_source['wiki'], lines=True, chunksize=int(1e5)),\n",
    "    'legal': pd.read_json(passages_source['legal'], lines=True, chunksize=int(1e5)),\n",
    "}"
   ],
   "id": "4fd5ec25acfe9457",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Trim wiki-trivia dataset\n",
    "\n",
    "- use `sample()` function from `pandas` in order to get simple random sample (~40k passages in total)\n",
    "- preserve positive matches contained in supplied files"
   ],
   "id": "3bb8fb462dfc7fb0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import csv\n",
    "\n",
    "# Gather positive matches\n",
    "wiki_positives_1 = pd.read_csv(os.path.join(dataset_path + 'wiki/', 'pairs-test.tsv'), sep='\\t')\n",
    "# wiki_positives_2 = pd.read_csv(os.path.join(dataset_path + 'dev-0/', 'expected.tsv'), sep='\\t', header=None)\n",
    "\n",
    "passage_ids = []\n",
    "\n",
    "print(wiki_positives_1)\n",
    "\n",
    "\n",
    "for _, row in wiki_positives_1.iterrows():\n",
    "    if row['passage-id'] not in passage_ids:\n",
    "        passage_ids.append(row['passage-id'])\n",
    "        \n",
    "with open(os.path.join(dataset_path + 'dev-0/', 'expected.tsv'), newline='') as wiki_positives_2:\n",
    "    reader = csv.reader(wiki_positives_2, delimiter='\\t', quotechar='\"')\n",
    "    for row in reader:\n",
    "        for entry in row:\n",
    "            if entry not in passage_ids:\n",
    "                passage_ids.append(entry)\n",
    "        \n",
    "print(len(passage_ids))"
   ],
   "id": "eba8c5b69063d63b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Write selected passages to file\n",
    "for passages_batch in tqdm(pd.read_json(passages_source['wiki'], lines=True, chunksize=int(1e6))):\n",
    "    positives = passages_batch[passages_batch[\"id\"].isin(passage_ids)]\n",
    "    sample_batch = pd.concat([positives, passages_batch.sample(random_state=1234, frac=0.005)], verify_integrity=True, ignore_index=True)\n",
    "    sample_batch.to_json(os.path.join(dataset_path + 'wiki/', 'passages-trimmed.jl'), index=False, orient='records', mode='a', lines=True, force_ascii=False)"
   ],
   "id": "5e5e2ffb63607faf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set new source file\n",
    "passages_source['wiki'] = os.path.join(dataset_path + 'wiki/', 'passages-trimmed.jl')\n",
    "print(passages_source['wiki'])"
   ],
   "id": "a5edcf00af5c62e5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "delete_documents()\n",
    "\n",
    "def write_to_document_store(key):\n",
    "    for batch in tqdm(passages[key]):\n",
    "        if 'title' in batch:\n",
    "            batch['title'] = batch['title'].fillna('')\n",
    "            batch['text'] = batch.apply(lambda r: r['title'] + ' ' + r['text'], axis=1)\n",
    "        \n",
    "        document_stores[key].write_documents([Document(content=str(item['text']), id=str(item['id'])) for item in batch.to_dict(orient='records')], policy=DuplicatePolicy.SKIP)\n",
    "\n",
    "write_to_document_store('allegro')\n",
    "write_to_document_store('legal')\n",
    "# Takes ~4h\n",
    "write_to_document_store('wiki')"
   ],
   "id": "e189d736",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load questions",
   "id": "56a243269cf0a387"
  },
  {
   "cell_type": "code",
   "id": "662d7b96",
   "metadata": {},
   "source": [
    "import csv\n",
    "\n",
    "questions = {\n",
    "    'allegro': pd.read_json(questions_source['allegro'], lines=True),\n",
    "    'wiki': pd.read_json(questions_source['wiki'], lines=True),\n",
    "    'legal': pd.read_json(questions_source['legal'], lines=True),\n",
    "    'dev-0': pd.read_csv(questions_source['dev-0'], sep='\\t', header=None).rename(columns={0: 'dataset', 1: 'text'}),\n",
    "    'test-A': pd.read_csv(questions_source['test-A'], sep='\\t', header=None).rename(columns={0: 'dataset', 1: 'text'}),\n",
    "    'test-B': pd.read_csv(questions_source['test-B'], sep='\\t', header=None).rename(columns={0: 'dataset', 1: 'text'}),\n",
    "}\n",
    "\n",
    "print(questions['allegro'].shape)\n",
    "print(questions['wiki'].shape)\n",
    "print(questions['legal'].shape)\n",
    "print(questions['dev-0'].shape)\n",
    "print(questions['test-A'].shape)\n",
    "print(questions['test-B'].shape)\n",
    "\n",
    "pairs_test = {\n",
    "    'allegro': pd.read_csv(os.path.join(dataset_path, 'allegro' + '/pairs-test.tsv'), sep='\\t'),\n",
    "    'wiki': pd.read_csv(os.path.join(dataset_path, 'wiki' + '/pairs-test.tsv'), sep='\\t'),\n",
    "    'legal': pd.read_csv(os.path.join(dataset_path, 'legal' + '/pairs-test.tsv'), sep='\\t'),    \n",
    "}\n",
    "\n",
    "print(pairs_test['wiki'].shape)\n",
    "print(pairs_test['legal'].shape)\n",
    "print(pairs_test['allegro'].shape)\n",
    "\n",
    "def format_test_inputs(_question_key):\n",
    "    df = {\n",
    "        'allegro': pd.DataFrame(columns=['question-id', 'passage-id', 'score']),\n",
    "        'wiki': pd.DataFrame(columns=['question-id', 'passage-id', 'score']),\n",
    "        'legal': pd.DataFrame(columns=['question-id', 'passage-id', 'score']), \n",
    "    }\n",
    "    df_questions_test = {\n",
    "        'allegro': pd.DataFrame(columns=['id', 'text']),\n",
    "        'wiki': pd.DataFrame(columns=['id', 'text']),\n",
    "        'legal': pd.DataFrame(columns=['id', 'text']), \n",
    "    }\n",
    "    \n",
    "    for index, _row in questions[_question_key].iterrows():\n",
    "        _key = _row['dataset'].split('-')[0].strip()\n",
    "        found_docs = questions[_key][questions[_key]['text'].apply(str.strip) == _row['text'].strip()]\n",
    "        if len(found_docs) == 0:\n",
    "            print('Could not find question with such content', _key, _row['text'])\n",
    "            return\n",
    "        df_questions_test[_key].loc[int(len(df_questions_test[_key]))] = {'id': found_docs.iloc[0]['id'], 'text': found_docs.iloc[0]['text']}\n",
    "        found_positives_for_questions = pairs_test[_key][pairs_test[_key]['question-id'] == found_docs.iloc[0]['id']]\n",
    "        for _, __row in found_positives_for_questions.iterrows():\n",
    "            df[_key].loc[int(len(df[_key]))] = {'question-id': __row['question-id'], 'passage-id': __row['passage-id'], 'score': 1}\n",
    "            \n",
    "    for _key in df:\n",
    "        df[_key].to_csv(os.path.join(dataset_path, _question_key + '/' + _key + '_chunk' + '/pairs-test.tsv'), sep='\\t', index=False)\n",
    "        df_questions_test[_key].to_json(os.path.join(dataset_path, _question_key + '/' + _key + '_chunk' + '/questions-test.jl'), orient='records', lines=True, force_ascii=False)\n",
    "    \n",
    "    return df, df_questions_test\n",
    "\n",
    "df_pairs_test_testA, df_questions_testA = format_test_inputs('test-A')\n",
    "df_pairs_test_testB, df_questions_testB = format_test_inputs('test-B')\n",
    "\n",
    "def convert_dev_input_to_common_format():\n",
    "    df_dev = pd.DataFrame(columns=['question-id', 'passage-id', 'score'])\n",
    "    df_questions_dev = pd.DataFrame(columns=['id', 'text'])\n",
    "    \n",
    "    with open(os.path.join(dataset_path + 'dev-0/', 'expected.tsv'), newline='') as wiki_positives_dev:\n",
    "        dev_reader = csv.reader(wiki_positives_dev, delimiter='\\t', quotechar='\"')\n",
    "        for (index, question_row), passage_row in zip(questions['dev-0'].iterrows(), dev_reader):\n",
    "            for passage_entry in passage_row:\n",
    "                df_dev.loc[int(len(df_dev))] = {'question-id': index, 'passage-id': passage_entry, 'score': 1}\n",
    "                \n",
    "            df_questions_dev.loc[int(len(df_questions_dev))] = {'id': index, 'text': question_row['text']}\n",
    "            \n",
    "    df_questions_dev.to_json(os.path.join(dataset_path, 'dev-0/questions-test.jl'), orient='records', lines=True, force_ascii=False)\n",
    "    df_dev.to_csv(os.path.join(dataset_path, 'dev-0/pairs-test.tsv'), sep='\\t', index=False)\n",
    "    \n",
    "    return df_dev, df_questions_dev\n",
    "\n",
    "df_pairs_test_dev0, df_questions_dev0 = convert_dev_input_to_common_format()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Construct pipes and gather predictions",
   "id": "b0ea5677"
  },
  {
   "cell_type": "code",
   "id": "ae70ea76",
   "metadata": {},
   "source": [
    "def run_pipe(pipeline, pipe_param_callback, _questions):\n",
    "    preds = []\n",
    "    \n",
    "    for _, row in _questions.iterrows():\n",
    "        pipe_params = pipe_param_callback(row)\n",
    "        top_passages = pipeline.run(pipe_params)\n",
    "        \n",
    "        for passage in top_passages['retriever']['documents']:\n",
    "            passage = passage.to_dict()\n",
    "            preds.append({\n",
    "                'question-id': row['id'],\n",
    "                'passage-id': passage['id'],\n",
    "                'score': passage['score'],\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(preds)\n",
    "\n",
    "def run_pipe_test(pipelines, pipe_param_callback, _questions, _questions_test=None):\n",
    "    preds = {\n",
    "        'wiki': [],\n",
    "        'legal': [],\n",
    "        'allegro': [],\n",
    "    }\n",
    "    indices = {\n",
    "        'wiki': 0,\n",
    "        'legal': 0,\n",
    "        'allegro': 0,\n",
    "    }\n",
    "    \n",
    "    for index, row in _questions.iterrows():\n",
    "        pipe_params = pipe_param_callback(row)\n",
    "        _key = row['dataset'].split('-')[0].strip()\n",
    "        top_passages = pipelines[_key].run(pipe_params)\n",
    "        question_id = _questions_test[_key].iloc[indices[_key]]['id'] if _questions_test is not None else index\n",
    "        if len(top_passages['retriever']['documents']) <= 1:\n",
    "            print(question_id, pipe_params)\n",
    "        \n",
    "        for passage in top_passages['retriever']['documents']:\n",
    "            passage = passage.to_dict()\n",
    "            preds[_key].append({\n",
    "                'question-id': question_id,\n",
    "                'passage-id': passage['id'],\n",
    "                'score': passage['score'],\n",
    "            })\n",
    "        \n",
    "        indices[_key] += 1\n",
    "    \n",
    "    return {\n",
    "        'wiki': pd.DataFrame(preds['wiki']),\n",
    "        'legal': pd.DataFrame(preds['legal']),\n",
    "        'allegro': pd.DataFrame(preds['allegro']),\n",
    "    }\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Base BM25 algorithm\n",
    "\n",
    "Uses a retriever that utilizes BM25 algorithm (bag-of-words based)"
   ],
   "id": "b39e01de7e96a1eb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "bm25_retrievers = {\n",
    "    'allegro': ElasticsearchBM25Retriever(document_store=document_stores['allegro']),\n",
    "    'wiki': ElasticsearchBM25Retriever(document_store=document_stores['wiki']),\n",
    "    'legal': ElasticsearchBM25Retriever(document_store=document_stores['legal']),\n",
    "}\n",
    "\n",
    "def construct_bm25_pipelines():\n",
    "    pipes = {\n",
    "        'allegro': Pipeline(),\n",
    "        'wiki': Pipeline(),\n",
    "        'legal': Pipeline(),\n",
    "    }\n",
    "    \n",
    "    pipes['allegro'].add_component(\"retriever\", bm25_retrievers['allegro'])\n",
    "    pipes['wiki'].add_component(\"retriever\", bm25_retrievers['wiki'])\n",
    "    pipes['legal'].add_component(\"retriever\", bm25_retrievers['legal'])\n",
    "    \n",
    "    return pipes\n",
    "\n",
    "bm25_pipes = construct_bm25_pipelines()\n",
    "bm25_pipe_param_callback = lambda row: { 'retriever': {\"query\": row['text']}}\n",
    "\n",
    "bm25_predictions = {\n",
    "    'discriminator': 'bm25',\n",
    "    'allegro': run_pipe(bm25_pipes['allegro'], bm25_pipe_param_callback, questions['allegro']),\n",
    "    'legal': run_pipe(bm25_pipes['legal'], bm25_pipe_param_callback, questions['legal']),\n",
    "    'wiki': run_pipe(bm25_pipes['wiki'], bm25_pipe_param_callback, questions['wiki']),\n",
    "    'dev-0': run_pipe_test(bm25_pipes, bm25_pipe_param_callback, questions['dev-0']),\n",
    "    'test-A': run_pipe_test(bm25_pipes, bm25_pipe_param_callback, questions['test-A'], df_questions_testA),\n",
    "    'test-B': run_pipe_test(bm25_pipes, bm25_pipe_param_callback, questions['test-B'], df_questions_testB),\n",
    "}"
   ],
   "id": "8b494b4c306b66af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### BM25 with spacy lemmatization\n",
    "\n",
    "- use a large pretrained polish model\n",
    "- include common polish stopwords"
   ],
   "id": "4b992922abcd5813"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Lemmatization setup",
   "id": "e4aa2ff783df03"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import spacy\n",
    "from constants import STOPWORDS_1\n",
    "\n",
    "# Load largest spaCy pipeline for Polish\n",
    "# Make sure to run: python -m spacy download pl_core_news_lg\n",
    "spacy_model = \"pl_core_news_lg\"\n",
    "\n",
    "nlp = spacy.load(spacy_model)\n",
    "\n",
    "for stopword in STOPWORDS_1:\n",
    "    nlp.vocab[stopword].is_stop = True\n",
    "    \n",
    "print(' '.join([token.lemma_ for token in nlp(\"Pojechałem do dziadków.\") if not token.is_stop]))"
   ],
   "id": "eaab6c416207b9ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "passages = {\n",
    "    'allegro': pd.read_json(passages_source['allegro'], lines=True, chunksize=int(1e5)),\n",
    "    'wiki': pd.read_json(passages_source['wiki'], lines=True, chunksize=int(1e5)),\n",
    "    'legal': pd.read_json(passages_source['legal'], lines=True, chunksize=int(1e5)),\n",
    "}"
   ],
   "id": "72dcb076fd8abce1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "delete_documents(document_stores['wiki'])",
   "id": "a72311e48c9ed220",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def write_to_document_store_l(key):\n",
    "    for batch in tqdm(passages[key]):\n",
    "        if 'title' in batch:\n",
    "            batch['title'] = batch['title'].fillna('')\n",
    "            batch['text'] = batch.apply(lambda r: r['title'] + ' ' + r['text'], axis=1)\n",
    "        \n",
    "        document_stores[key].write_documents([Document(content=' '.join([token.lemma_ for token in nlp(item['text']) if not token.is_stop]), id=str(item['id'])) for item in batch.to_dict(orient='records')], policy=DuplicatePolicy.SKIP)\n",
    "\n",
    "write_to_document_store_l('allegro')\n",
    "write_to_document_store_l('legal')\n",
    "write_to_document_store_l('wiki')"
   ],
   "id": "196caa5c9e5b3289",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "bm25_l_pipe_param_callback = lambda _row: { 'retriever': {\"query\": ' '.join([token.lemma_ for token in nlp(_row['text']) if not token.is_stop])}}\n",
    "\n",
    "bm25_l_predictions = {\n",
    "    'discriminator': 'bm25-l',\n",
    "    'allegro': run_pipe(bm25_pipes['allegro'], bm25_l_pipe_param_callback, questions['allegro']),\n",
    "    'legal': run_pipe(bm25_pipes['legal'], bm25_l_pipe_param_callback, questions['legal']),\n",
    "    'wiki': run_pipe(bm25_pipes['wiki'], bm25_l_pipe_param_callback, questions['wiki']),\n",
    "    'dev-0': run_pipe_test(bm25_pipes, bm25_l_pipe_param_callback, questions['dev-0']),\n",
    "    'test-A': run_pipe_test(bm25_pipes, bm25_l_pipe_param_callback, questions['test-A'], df_questions_testA),\n",
    "    'test-B': run_pipe_test(bm25_pipes, bm25_l_pipe_param_callback, questions['test-B'], df_questions_testB),\n",
    "}"
   ],
   "id": "4070961b86b1730d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Text embedding",
   "id": "9b7cef8101dad37"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "### Definitions, functions\n",
    "from haystack_integrations.components.retrievers.elasticsearch import ElasticsearchEmbeddingRetriever\n",
    "from haystack.components.embedders import SentenceTransformersTextEmbedder\n",
    "from haystack.components.embedders import SentenceTransformersDocumentEmbedder\n",
    "\n",
    "model_mpnet = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_minilm = \"sentence-transformers/all-MiniLM-L12-v2\"\n",
    "model_roberta_large = \"sdadas/mmlw-retrieval-roberta-large\"\n",
    "\n",
    "mppnet_document_embedder = SentenceTransformersDocumentEmbedder(model=model_mpnet)  \n",
    "mppnet_document_embedder.warm_up()\n",
    "\n",
    "minilm_document_embedder = SentenceTransformersDocumentEmbedder(model=model_minilm)  \n",
    "minilm_document_embedder.warm_up()\n",
    "\n",
    "roberta_document_embedder = SentenceTransformersDocumentEmbedder(model=model_roberta_large)\n",
    "roberta_document_embedder.warm_up()\n",
    "\n",
    "def write_to_document_store_e(key, document_embedder):\n",
    "    for batch in tqdm(passages[key]):\n",
    "        if 'title' in batch:\n",
    "            batch['title'] = batch['title'].fillna('')\n",
    "            batch['text'] = batch.apply(lambda r: r['title'] + ' ' + r['text'], axis=1)\n",
    "        \n",
    "        batch = batch.rename(columns={'id': 'passage-id', 'text': 'content'})\n",
    "        batch_as_dicts = batch.to_dict(orient='records')\n",
    "        documents_list = [Document(content=str(passageDict['content']), id=str(passageDict['passage-id'])) for passageDict in batch_as_dicts]\n",
    "        documents_with_embeddings = document_embedder.run(documents_list)\n",
    "        document_stores[key].write_documents(documents_with_embeddings['documents'], policy=DuplicatePolicy.SKIP)\n",
    "        \n",
    "embedding_retrievers = {\n",
    "    'allegro': ElasticsearchEmbeddingRetriever(document_store=document_stores['allegro']),\n",
    "    'wiki': ElasticsearchEmbeddingRetriever(document_store=document_stores['wiki']),\n",
    "    'legal': ElasticsearchEmbeddingRetriever(document_store=document_stores['legal']),\n",
    "}\n",
    "        \n",
    "def construct_embedding_pipelines(text_embedders):\n",
    "    pipes = {\n",
    "        'allegro': Pipeline(),\n",
    "        'wiki': Pipeline(),\n",
    "        'legal': Pipeline(),\n",
    "    }\n",
    "    \n",
    "    pipes['allegro'].add_component(\"text_embedder\", text_embedders['allegro'])\n",
    "    pipes['allegro'].add_component(\"retriever\", embedding_retrievers['allegro'])\n",
    "    pipes['allegro'].connect(\"text_embedder.embedding\", \"retriever.query_embedding\")\n",
    "    \n",
    "    pipes['wiki'].add_component(\"text_embedder\", text_embedders['wiki'])\n",
    "    pipes['wiki'].add_component(\"retriever\", embedding_retrievers['wiki'])\n",
    "    pipes['wiki'].connect(\"text_embedder.embedding\", \"retriever.query_embedding\")\n",
    "    \n",
    "    pipes['legal'].add_component(\"text_embedder\", text_embedders['legal'])\n",
    "    pipes['legal'].add_component(\"retriever\", embedding_retrievers['legal'])\n",
    "    pipes['legal'].connect(\"text_embedder.embedding\", \"retriever.query_embedding\")\n",
    "    \n",
    "    return pipes"
   ],
   "id": "8232331360adbf16",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### mpnet-base-v2 model\n",
    "\n",
    "- pretrained\n",
    "- slow\n",
    "- general use case, multilingual"
   ],
   "id": "3f1bc860a2a7057e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "passages = {\n",
    "    'allegro': pd.read_json(passages_source['allegro'], lines=True, chunksize=int(3e6)),\n",
    "    'wiki': pd.read_json(passages_source['wiki'], lines=True, chunksize=int(3e6)),\n",
    "    'legal': pd.read_json(passages_source['legal'], lines=True, chunksize=int(3e6)),\n",
    "}"
   ],
   "id": "f681a22f4c3211f3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "delete_documents()\n",
    "\n",
    "write_to_document_store_e('allegro', mppnet_document_embedder)\n",
    "write_to_document_store_e('legal', mppnet_document_embedder)\n",
    "write_to_document_store_e('wiki', mppnet_document_embedder)\n",
    "\n",
    "mpnet_text_embedders = {\n",
    "    'allegro': SentenceTransformersTextEmbedder(model=model_mpnet, progress_bar=False),\n",
    "    'wiki': SentenceTransformersTextEmbedder(model=model_mpnet, progress_bar=False),\n",
    "    'legal': SentenceTransformersTextEmbedder(model=model_mpnet, progress_bar=False),\n",
    "}\n",
    "\n",
    "mpnet_pipes = construct_embedding_pipelines(mpnet_text_embedders)\n",
    "mpnet_pipe_param_callback = lambda row: {'text_embedder': {'text': row['text']}}\n",
    "\n",
    "mpnet_predictions = {\n",
    "    'discriminator': 'mpnet',\n",
    "    'allegro': run_pipe(mpnet_pipes['allegro'], mpnet_pipe_param_callback, questions['allegro']),\n",
    "    'dev-0': run_pipe_test(mpnet_pipes, mpnet_pipe_param_callback, questions['dev-0']),\n",
    "    'legal': run_pipe(mpnet_pipes['legal'], mpnet_pipe_param_callback, questions['legal']),\n",
    "    'test-A': run_pipe_test(mpnet_pipes, mpnet_pipe_param_callback, questions['test-A'], df_questions_testA),\n",
    "    'wiki': run_pipe(mpnet_pipes['wiki'], mpnet_pipe_param_callback, questions['wiki']),\n",
    "    'test-B': run_pipe_test(mpnet_pipes, mpnet_pipe_param_callback, questions['test-B'], df_questions_testB),\n",
    "}"
   ],
   "id": "651d089743d24eed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(mpnet_predictions)",
   "id": "d3ccc3da4e94dfaf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### MiniLM-L12-v2\n",
    "\n",
    "- faster\n",
    "- general use case, multilingual\n",
    "- pretrained"
   ],
   "id": "3c098d1656f3f8b4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "passages = {\n",
    "    'allegro': pd.read_json(passages_source['allegro'], lines=True, chunksize=int(2e6)),\n",
    "    'wiki': pd.read_json(passages_source['wiki'], lines=True, chunksize=int(2e6)),\n",
    "    'legal': pd.read_json(passages_source['legal'], lines=True, chunksize=int(2e6)),\n",
    "}"
   ],
   "id": "969ee4a464fd2304",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "count_documents(document_stores)",
   "id": "90e27c676539ac7b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "delete_documents()\n",
    "\n",
    "write_to_document_store_e('allegro', minilm_document_embedder)\n",
    "write_to_document_store_e('legal', minilm_document_embedder)\n",
    "write_to_document_store_e('wiki', minilm_document_embedder)\n",
    "\n",
    "minilm_text_embedders = {\n",
    "    'allegro': SentenceTransformersTextEmbedder(model=model_minilm, progress_bar=False),\n",
    "    'wiki': SentenceTransformersTextEmbedder(model=model_minilm, progress_bar=False),\n",
    "    'legal': SentenceTransformersTextEmbedder(model=model_minilm, progress_bar=False),\n",
    "}\n",
    "\n",
    "minilm_pipes = construct_embedding_pipelines(minilm_text_embedders)\n",
    "minilm_pipe_param_callback = lambda row: {'text_embedder': {'text': row['text']}}\n",
    "\n",
    "minilm_predictions = {\n",
    "    'discriminator': 'MiniLM',\n",
    "    'allegro': run_pipe(minilm_pipes['allegro'], minilm_pipe_param_callback, questions['allegro']),\n",
    "    'legal': run_pipe(minilm_pipes['legal'], minilm_pipe_param_callback, questions['legal']),\n",
    "    'wiki': run_pipe(minilm_pipes['wiki'], minilm_pipe_param_callback, questions['wiki']),\n",
    "    'dev-0': run_pipe_test(minilm_pipes, minilm_pipe_param_callback, questions['dev-0']),\n",
    "    'test-A': run_pipe_test(minilm_pipes, minilm_pipe_param_callback, questions['test-A'], df_questions_testA),\n",
    "    'test-B': run_pipe_test(minilm_pipes, minilm_pipe_param_callback, questions['test-B'], df_questions_testB),\n",
    "}"
   ],
   "id": "1ce589f181cd353a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(minilm_predictions)",
   "id": "a4a07b566f31bb3f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### roberta-large\n",
    "\n",
    "- trained on polish specifically\n",
    "- large size (~1gb)\n",
    "- slowest\n",
    "- author: Slawomir Dadas"
   ],
   "id": "eb61a56d106da01c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "passages = {\n",
    "    'allegro': pd.read_json(passages_source['allegro'], lines=True, chunksize=int(2e6)),\n",
    "    'wiki': pd.read_json(passages_source['wiki'], lines=True, chunksize=int(2e6)),\n",
    "    'legal': pd.read_json(passages_source['legal'], lines=True, chunksize=int(2e6)),\n",
    "}"
   ],
   "id": "fa685addb21692ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "delete_documents()\n",
    "\n",
    "write_to_document_store_e('allegro', roberta_document_embedder)\n",
    "write_to_document_store_e('legal', roberta_document_embedder)\n",
    "write_to_document_store_e('wiki', roberta_document_embedder)\n",
    "\n",
    "roberta_text_embedders = {\n",
    "    'allegro': SentenceTransformersTextEmbedder(model=model_roberta_large, progress_bar=False),\n",
    "    'wiki': SentenceTransformersTextEmbedder(model=model_roberta_large, progress_bar=False),\n",
    "    'legal': SentenceTransformersTextEmbedder(model=model_roberta_large, progress_bar=False),\n",
    "}\n",
    "\n",
    "roberta_pipes = construct_embedding_pipelines(roberta_text_embedders)\n",
    "roberta_pipe_param_callback = lambda row: {'text_embedder': {'text': \"zapytanie: \" + row['text']}}\n",
    "\n",
    "roberta_predictions = {\n",
    "    'discriminator': 'roberta',\n",
    "    'allegro': run_pipe(roberta_pipes['allegro'], roberta_pipe_param_callback, questions['allegro']),\n",
    "    'legal': run_pipe(roberta_pipes['legal'], roberta_pipe_param_callback, questions['legal']),\n",
    "    'wiki': run_pipe(roberta_pipes['wiki'], roberta_pipe_param_callback, questions['wiki']),\n",
    "    'dev-0': run_pipe_test(roberta_pipes, roberta_pipe_param_callback, questions['dev-0']),\n",
    "    'test-A': run_pipe_test(roberta_pipes, roberta_pipe_param_callback, questions['test-A'], df_questions_testA),\n",
    "    'test-B': run_pipe_test(roberta_pipes, roberta_pipe_param_callback, questions['test-B'], df_questions_testB),\n",
    "}"
   ],
   "id": "ef7fbc26ed784ef9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(roberta_predictions)",
   "id": "5d96ffe1ea8471f6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### *spacy mean word embeddings\n",
    "\n",
    "NOT CONSIDERED IN PROJECT. Very bad results because it takes the mean vector of embeddings of all words in a sentence."
   ],
   "id": "1bd9b713677f382d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from custom_components import PolishSpacyTextEmbedder, PolishSpacyDocumentEmbedder\n",
    "\n",
    "spacy_document_embedder = PolishSpacyDocumentEmbedder(model=spacy_model)  \n",
    "spacy_document_embedder.warm_up()"
   ],
   "id": "2673c345eccdffb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "count_documents(stores=document_stores)",
   "id": "e6c8a2070e438870",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# delete_documents()\n",
    "\n",
    "write_to_document_store_e('allegro', spacy_document_embedder)\n",
    "write_to_document_store_e('legal', spacy_document_embedder)\n",
    "write_to_document_store_e('wiki', minilm_document_embedder)\n",
    "\n",
    "spacy_text_embedders = {\n",
    "    'allegro': PolishSpacyTextEmbedder(model=spacy_model),\n",
    "    'wiki': PolishSpacyTextEmbedder(model=spacy_model),\n",
    "    'legal': PolishSpacyTextEmbedder(model=spacy_model),\n",
    "}\n",
    "\n",
    "spacy_text_embedders['allegro'].warm_up()\n",
    "spacy_text_embedders['wiki'].warm_up()\n",
    "spacy_text_embedders['legal'].warm_up()\n",
    "\n",
    "spacy_pipes = construct_embedding_pipelines(spacy_text_embedders)\n",
    "spacy_pipe_param_callback = lambda row: {'text_embedder': {'text': row['text']}}\n",
    "\n",
    "spacy_predictions = {\n",
    "    'discriminator': 'spacy',\n",
    "    'allegro': run_pipe(spacy_pipes['allegro'], spacy_pipe_param_callback, questions['allegro']),\n",
    "    'legal': run_pipe(spacy_pipes['legal'], spacy_pipe_param_callback, questions['legal']),\n",
    "    # 'wiki': run_pipe(minilm_pipes['wiki'], minilm_pipe_param_callback, questions['wiki']),\n",
    "}\n"
   ],
   "id": "9f9f1af769a382d1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(spacy_predictions)",
   "id": "888ce4d90caf66a0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### *roberta-large with lemmatization\n",
    "\n",
    "NOT INCLUDED IN PROJECT RESULTS. sentence transformers capture the semantic meaning of paragraphs, and we are kinda removing this using lemmatization - results are significantly worse"
   ],
   "id": "32dd344fe20030cf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def write_to_document_store_l_e(key, document_embedder):\n",
    "    for batch in tqdm(passages[key]):\n",
    "        if 'title' in batch:\n",
    "            batch['title'] = batch['title'].fillna('')\n",
    "            batch['text'] = batch.apply(lambda r: r['title'] + ' ' + r['text'], axis=1)\n",
    "        \n",
    "        batch = batch.rename(columns={'id': 'passage-id', 'text': 'content'})\n",
    "        batch_as_dicts = batch.to_dict(orient='records')\n",
    "        documents_list = [Document(content=' '.join([token.lemma_ for token in nlp(passageDict['content']) if not token.is_stop]), id=str(passageDict['passage-id'])) for passageDict in batch_as_dicts]\n",
    "        documents_with_embeddings = document_embedder.run(documents_list)\n",
    "        document_stores[key].write_documents(documents_with_embeddings['documents'], policy=DuplicatePolicy.SKIP)"
   ],
   "id": "e9e61b17e4f79769",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "passages = {\n",
    "    'allegro': pd.read_json(passages_source['allegro'], lines=True, chunksize=int(2e6)),\n",
    "    'wiki': pd.read_json(passages_source['wiki'], lines=True, chunksize=int(2e6)),\n",
    "    'legal': pd.read_json(passages_source['legal'], lines=True, chunksize=int(2e6)),\n",
    "}"
   ],
   "id": "27beb0dde18753ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "delete_documents()\n",
    "\n",
    "write_to_document_store_l_e('allegro', roberta_document_embedder)\n",
    "write_to_document_store_l_e('legal', roberta_document_embedder)\n",
    "write_to_document_store_l_e('wiki', roberta_document_embedder)\n",
    "\n",
    "roberta_text_embedders = {\n",
    "    'allegro': SentenceTransformersTextEmbedder(model=model_roberta_large, progress_bar=False),\n",
    "    'wiki': SentenceTransformersTextEmbedder(model=model_roberta_large, progress_bar=False),\n",
    "    'legal': SentenceTransformersTextEmbedder(model=model_roberta_large, progress_bar=False),\n",
    "}\n",
    "\n",
    "roberta_pipes = construct_embedding_pipelines(roberta_text_embedders)\n",
    "roberta_l_pipe_param_callback = lambda row: {'text_embedder': {'text': \"zapytanie: \" + ' '.join([token.lemma_ for token in nlp(row['text']) if not token.is_stop])}}\n",
    "\n",
    "roberta_l_predictions = {\n",
    "    'discriminator': 'roberta-l',\n",
    "    'allegro': run_pipe(roberta_pipes['allegro'], roberta_l_pipe_param_callback, questions['allegro']),\n",
    "    'legal': run_pipe(roberta_pipes['legal'], roberta_l_pipe_param_callback, questions['legal']),\n",
    "    'wiki': run_pipe(roberta_pipes['wiki'], roberta_l_pipe_param_callback, questions['wiki']),\n",
    "    'dev-0': run_pipe_test(roberta_pipes, roberta_l_pipe_param_callback, questions['dev-0']),\n",
    "    'test-A': run_pipe_test(roberta_pipes, roberta_l_pipe_param_callback, questions['test-A'], df_questions_testA),\n",
    "    'test-B': run_pipe_test(roberta_pipes, roberta_l_pipe_param_callback, questions['test-B'], df_questions_testB),\n",
    "}"
   ],
   "id": "9cba81fdf146c57e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Train roberta-large \n",
    "\n",
    "- best performing model\n",
    "- train using provided training/test sets"
   ],
   "id": "9f692b03ae6e2f55"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(training_set)\n",
    "print(test_set)"
   ],
   "id": "efaf6dc3b3685b16",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "passages = {\n",
    "    'allegro': pd.read_json(passages_source['allegro'], lines=True, chunksize=int(3e6)),\n",
    "    'wiki': pd.read_json(passages_source['wiki'], lines=True, chunksize=int(3e6)),\n",
    "    'legal': pd.read_json(passages_source['legal'], lines=True, chunksize=int(3e6)),\n",
    "}"
   ],
   "id": "d03bebefcf8be337",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sentence_transformers import SentenceTransformer, losses\n",
    "\n",
    "model_roberta_large = \"sdadas/mmlw-retrieval-roberta-large\"\n",
    "roberta_train = SentenceTransformer(model_roberta_large)\n",
    "roberta_train_loss = losses.MultipleNegativesRankingLoss(model=roberta_train)"
   ],
   "id": "638d124381bec000",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(training_set)\n",
    "print(training_set.shape)\n"
   ],
   "id": "8659580d35c882c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sentence_transformers import InputExample\n",
    "\n",
    "train_examples = []\n",
    "n_examples = training_set.num_rows\n",
    "\n",
    "wiki_questions = pd.read_json(wiki_questions_train, lines=True)\n",
    "inner_passages = pd.read_json(passages_source['wiki'], lines=True, chunksize=int(5e6))\n",
    "\n",
    "processed_indices = []\n",
    "\n",
    "for passage_batch in tqdm(inner_passages):\n",
    "    if 'title' in passage_batch:\n",
    "            passage_batch['title'] = passage_batch['title'].fillna('')\n",
    "            passage_batch['text'] = passage_batch.apply(lambda r: r['title'] + ' ' + r['text'], axis=1)\n",
    "    \n",
    "    gen = (index for index in tqdm(range(n_examples)) if index not in processed_indices)\n",
    "    for index in gen:\n",
    "        found_passage = passage_batch[passage_batch['id'] == training_set[index]['passage-id']]\n",
    "        \n",
    "        if len(found_passage) == 1:\n",
    "            processed_indices.append(index)\n",
    "            found_question = wiki_questions[wiki_questions['id'] == training_set[index]['question-id']]\n",
    "            \n",
    "            if len(found_question) != 1:\n",
    "                print(\"Error!!!\")\n",
    "                continue\n",
    "            else:\n",
    "                train_examples.append(InputExample(guid=index, texts=[found_question.iloc[0]['text'], found_passage.iloc[0]['text']]))"
   ],
   "id": "d156242092567f47",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(train_examples)",
   "id": "1866af5b4c17260f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "f = open(os.path.join(dataset_path, 'wiki/wiki-train-examples.jl'), mode='w', encoding='utf-8')\n",
    "\n",
    "json.dump([{'question': item.texts[0], 'passage': item.texts[1]} for item in train_examples], f, ensure_ascii=False)\n",
    "\n",
    "f.close()"
   ],
   "id": "cf8b5ef5e7892580",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sentence_transformers import InputExample\n",
    "import json\n",
    "\n",
    "f = open(os.path.join(dataset_path, 'wiki/wiki-train-examples.jl'), mode='r', encoding='utf-8')\n",
    "\n",
    "train_examples = [InputExample(guid=index, texts=[item['question'], item['passage']]) for index, item in enumerate(json.load(f))]\n",
    "\n",
    "f.close()\n",
    "print(train_examples[:4])"
   ],
   "id": "1282bf286e50f243",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "ba4532578f5f6f88"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=4)\n",
    "\n",
    "roberta_train.get_max_seq_length()"
   ],
   "id": "d351c5ae9a65ba45",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def main() -> None:\n",
    "    pool = roberta_train.start_multi_process_pool()\n",
    "    \n",
    "    roberta_train.fit(train_objectives=[(train_dataloader, roberta_train_loss)], epochs=1)\n",
    "    \n",
    "    roberta_train.stop_multi_process_pool(pool)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "9f98aaa75caf2e8d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "roberta_train.push_to_hub(\n",
    "    repo_id=\"szmarkiewicz/mmlw-retrieval-roberta-large-poleval\",\n",
    "    token=\"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\",\n",
    "    commit_message=\"roberta-large trained with poleval training set\",\n",
    "    train_datasets=[\"piotr-rybak/poleval2022-passage-retrieval-dataset\"],\n",
    "    exist_ok=True\n",
    "    )"
   ],
   "id": "3094275c12906f28",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### roberta-large trained results",
   "id": "2ed7b93bded3faef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "delete_documents()\n",
    "\n",
    "model_roberta_large_trained = \"szmarkiewicz/mmlw-retrieval-roberta-large-poleval\"\n",
    "\n",
    "roberta_trained_document_embedder = SentenceTransformersDocumentEmbedder(model=model_roberta_large_trained)\n",
    "roberta_trained_document_embedder.warm_up()\n",
    "\n",
    "write_to_document_store_e('allegro', roberta_trained_document_embedder)\n",
    "write_to_document_store_e('legal', roberta_trained_document_embedder)\n",
    "write_to_document_store_e('wiki', roberta_trained_document_embedder)\n",
    "\n",
    "roberta_trained_text_embedders = {\n",
    "    'allegro': SentenceTransformersTextEmbedder(model=model_roberta_large_trained, progress_bar=False),\n",
    "    'wiki': SentenceTransformersTextEmbedder(model=model_roberta_large_trained, progress_bar=False),\n",
    "    'legal': SentenceTransformersTextEmbedder(model=model_roberta_large_trained, progress_bar=False),\n",
    "}\n",
    "\n",
    "roberta_trained_pipes = construct_embedding_pipelines(roberta_trained_text_embedders)\n",
    "roberta_trained_pipe_param_callback = lambda row: {'text_embedder': {'text': \"zapytanie: \" + row['text']}}\n",
    "\n",
    "roberta_trained_predictions = {\n",
    "    'discriminator': 'roberta-trained',\n",
    "    'allegro': run_pipe(roberta_trained_pipes['allegro'], roberta_trained_pipe_param_callback, questions['allegro']),\n",
    "    'legal': run_pipe(roberta_trained_pipes['legal'], roberta_trained_pipe_param_callback, questions['legal']),\n",
    "    'wiki': run_pipe(roberta_trained_pipes['wiki'], roberta_trained_pipe_param_callback, questions['wiki']),\n",
    "    'dev-0': run_pipe_test(roberta_trained_pipes, roberta_trained_pipe_param_callback, questions['dev-0']),\n",
    "    'test-A': run_pipe_test(roberta_trained_pipes, roberta_trained_pipe_param_callback, questions['test-A'], df_questions_testA),\n",
    "    'test-B': run_pipe_test(roberta_trained_pipes, roberta_trained_pipe_param_callback, questions['test-B'], df_questions_testB),\n",
    "}"
   ],
   "id": "4408bb7f961e7dff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load into .tsv files",
   "id": "5fe732a4f2be09b0"
  },
  {
   "cell_type": "code",
   "id": "79be0f32",
   "metadata": {},
   "source": [
    "empty_predictions = {\n",
    "    'discriminator': 'bm25',\n",
    "    'allegro': 'allegro',\n",
    "    'legal': 'legal',\n",
    "    'wiki': 'wiki',\n",
    "    'dev-0': 'dev-0',\n",
    "    'test-A': {\n",
    "        'wiki': 'wiki',\n",
    "        'legal': 'legal',\n",
    "        'allegro': 'allegro',\n",
    "    },\n",
    "    'test-B': {\n",
    "        'wiki': 'wiki',\n",
    "        'legal': 'legal',\n",
    "        'allegro': 'allegro',\n",
    "    },\n",
    "}\n",
    "\n",
    "predictions = [\n",
    "    # empty_predictions,\n",
    "    # bm25_predictions,\n",
    "    # bm25_l_predictions,\n",
    "    # mpnet_predictions,\n",
    "    # minilm_predictions,\n",
    "    # spacy_predictions\n",
    "    # roberta_predictions,\n",
    "    # roberta_l_predictions,\n",
    "    roberta_trained_predictions,\n",
    "]\n",
    "\n",
    "for prediction in predictions:\n",
    "    if 'allegro' in prediction:\n",
    "        prediction['allegro'].to_csv(os.path.join(dataset_path + 'allegro/', prediction['discriminator'] + '/submission.tsv'), sep='\\t', index=False)\n",
    "    if 'wiki' in prediction:\n",
    "        prediction['wiki'].to_csv(os.path.join(dataset_path + 'wiki/', prediction['discriminator'] + '/submission.tsv'), sep='\\t', index=False)\n",
    "    if 'legal' in prediction:\n",
    "        prediction['legal'].to_csv(os.path.join(dataset_path + 'legal/', prediction['discriminator'] + '/submission.tsv'), sep='\\t', index=False)\n",
    "    if 'dev-0' in prediction:\n",
    "        for chunk_key in prediction['dev-0']:\n",
    "            if len(prediction['dev-0'][chunk_key]) > 0:\n",
    "                prediction['dev-0'][chunk_key].to_csv(os.path.join(dataset_path + 'dev-0/', prediction['discriminator'] + '/submission.tsv'), sep='\\t', index=False)\n",
    "    if 'test-A' in prediction:\n",
    "        for chunk_key in prediction['test-A']:\n",
    "            if len(prediction['test-A'][chunk_key]) > 0:\n",
    "                prediction['test-A'][chunk_key].to_csv(os.path.join(dataset_path + 'test-A/', prediction['discriminator'] + '/' + chunk_key + '/submission.tsv'), sep='\\t', index=False)\n",
    "    if 'test-B' in prediction:\n",
    "        for chunk_key in prediction['test-B']:\n",
    "            if len(prediction['test-B'][chunk_key]) > 0:\n",
    "                prediction['test-B'][chunk_key].to_csv(os.path.join(dataset_path + 'test-B/', prediction['discriminator'] + '/' + chunk_key + '/submission.tsv'), sep='\\t', index=False)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Evaluate\n",
    "\n",
    "`o evaluate the results, run these in terminal:`\n",
    "\n",
    "### allegro:\n",
    "`python ./eval.py --true datasets/allegro/pairs-test.tsv --pred datasets/allegro/<name_of_algorithm>/submission.tsv`\n",
    "\n",
    "### wiki\n",
    "`python ./eval.py --true datasets/wiki/pairs-test.tsv --pred datasets/wiki/<name_of_algorithm>/submission.tsv`\n",
    "\n",
    "### legal\n",
    "`python ./eval.py --true datasets/legal/pairs-test.tsv --pred datasets/legal/<name_of_algorithm>/submission.tsv`"
   ],
   "id": "4815b72b5377c7ff"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Merge results for test inputs",
   "id": "60872dacf13bf1cf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for prediction in predictions:\n",
    "    if 'test-A' in prediction:\n",
    "        results_sum = 0\n",
    "        for chunk_key in prediction['test-A']:\n",
    "            with open(os.path.join(dataset_path + 'test-A/', prediction['discriminator'] + '/' + chunk_key + '/results.txt')) as result:\n",
    "                results_sum += float(result.readline().split(' ')[1])\n",
    "        f = open(os.path.join(dataset_path + 'test-A/', prediction['discriminator'] + '/results.txt'), \"w\")\n",
    "        f.write(f'NDCG@10: {results_sum/3:.3f}')\n",
    "        f.close()\n",
    "    if 'test-B' in prediction:\n",
    "        results_sum = 0\n",
    "        for chunk_key in prediction['test-B']:\n",
    "            with open(os.path.join(dataset_path + 'test-B/', prediction['discriminator'] + '/' + chunk_key + '/results.txt')) as result:\n",
    "                results_sum += float(result.readline().split(' ')[1])\n",
    "        f2 = open(os.path.join(dataset_path + 'test-B/', prediction['discriminator'] + '/results.txt'), \"w\")\n",
    "        f2.write(f'NDCG@10: {results_sum/3:.3f}')\n",
    "        f2.close()"
   ],
   "id": "aba57ea55778a963",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "4c34d2e6a51630b3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Sources\n",
    "\n",
    "https://huggingface.co/blog/how-to-train-sentence-transformers\n"
   ],
   "id": "c5d59adbad5bd4b5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
